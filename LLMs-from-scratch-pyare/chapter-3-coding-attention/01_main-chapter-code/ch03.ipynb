{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f22086",
   "metadata": {},
   "source": [
    "# Chapter 03 : Coding Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70dcdb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\" torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9ed3a",
   "metadata": {},
   "source": [
    "\n",
    "Workflow till now\n",
    "\n",
    "<div align = \"center\">\n",
    "    <img src = \"../Ref_images/3.0.png\" center width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9073dde",
   "metadata": {},
   "source": [
    "- In this chapter we will code \n",
    "\n",
    "<img src=\"../Ref_images/3.2.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365de207",
   "metadata": {},
   "source": [
    "### 3.3.1 A Simple self-attention mechanism without trainable weights\n",
    "<img src =\"../Ref_images/3.7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a43f10",
   "metadata": {},
   "source": [
    "- Input sequence $x$, consisting of $T$ elements represented as $x^{(1)}$ to $x^{(T)}$\n",
    "\n",
    "Example: \"Your journey starts with one step\"\n",
    "- $x^{(1)}$, corresponds to a d-dimentional embedding vector representing a specific token, like \"Your\" has 3 dimentional embeddings.\n",
    "\n",
    "In **self-attention** your goal is to calculate context vectors $z^{(i)}$ for each element in the input sequence.\n",
    "\n",
    "- A ***Context Vector*** can be interpreted as an enriched embedding vector.\n",
    "\n",
    "- We will illustrate this with taking the word \"journey\" $x^{(2)}$ and the corresponding context vector $z^{(2)}$ which contains the infrmation about $x^{(2)}$ and all other input elements, $x^{(1)}$ to $x^{(T)}$.\n",
    "\n",
    "- First we will do simplified attention and later we will add trainable weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2890f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2) # query\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1448ff",
   "metadata": {},
   "source": [
    "- First step of implementing self-attention is to comppute the intermediate values *w*, referred as ***attention scores***.\n",
    "\n",
    "<img src=\"../Ref_images/3.8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf204ff9",
   "metadata": {},
   "source": [
    "### Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7d6c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] #  journey  (x^2)\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0]) # \n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111e39d",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
    "        x^1     x^2     x^3     x^4     x^5     x^6\n",
    "        Your   journey starts   with    one    step\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02a75f",
   "metadata": {},
   "source": [
    "### Understanding the dot product (bilinear - 2 vectors)\n",
    "An dot product is essentially a concise way of multiplying two vectors element wise and then summing the products. Which is demonstrated as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94efbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0] # Yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6556206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Standard Attention (dot product of 2 vectors) ===\n",
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "### Understanding the dot product (bilinear - 2 vectors)\n",
    "print(\"=== Standard Attention (dot product of 2 vectors) ===\")\n",
    "res = 0\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx]*query[idx] # element wise multiplication and addition\n",
    "print(res)\n",
    "print(torch.dot(inputs[0],query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c1eb2",
   "metadata": {},
   "source": [
    "#### 2-simplicial Attention scores: 2D scores (for single query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4ebe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2-Simplicial attention scores: \n",
      "tensor([[0.6441, 0.6313, 0.6217, 0.3216, 0.2735, 0.4393],\n",
      "        [0.6313, 1.1124, 1.0946, 0.6493, 0.4657, 0.8602],\n",
      "        [0.6217, 1.0946, 1.0776, 0.6373, 0.4685, 0.8396],\n",
      "        [0.3216, 0.6493, 0.6373, 0.3912, 0.2411, 0.5295],\n",
      "        [0.2735, 0.4657, 0.4685, 0.2411, 0.3871, 0.2315],\n",
      "        [0.4393, 0.8602, 0.8396, 0.5295, 0.2315, 0.7578]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2s = torch.empty(inputs.shape[0],inputs.shape[0]) # 6*6 = number of tokes * number of tokens\n",
    "for j, k_j in enumerate(inputs):\n",
    "    for k, k_prime_k in enumerate(inputs):\n",
    "        attn_scores_2s[j,k] = torch.sum(query* k_j* k_prime_k)\n",
    "\n",
    "print(\"\\n2-Simplicial attention scores: \")\n",
    "print(attn_scores_2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3790c",
   "metadata": {},
   "source": [
    "```\n",
    "                k=0     k=1     k=2     k=3     k=4     k=5\n",
    "                Your   journey starts   with    one    step\n",
    "            ┌──────────────────────────────────────────────  ┐\n",
    "j=0  Your   │ 0.6441  0.6313  0.6217  0.3216  0.2735  0.4393 │\n",
    "j=1 journey │ 0.6313  1.1124  1.0946  0.6493  0.4657  0.8602 │\n",
    "j=2 starts  │ 0.6217  1.0946  1.0776  0.6373  0.4685  0.8396 │\n",
    "j=3  with   │ 0.3216  0.6493  0.6373  0.3912  0.2411  0.5295 │\n",
    "j=4  one    │ 0.2735  0.4657  0.4685  0.2411  0.3871  0.2315 │\n",
    "j=5  step   │ 0.4393  0.8602  0.8396  0.5295  0.2315  0.7578 │\n",
    "            └──────────────────────────────────────────────  ┘\n",
    "```\n",
    "\n",
    "**Interpretation:** 36 scores (6×6) representing how much query (journey) attends to each pair of tokens.\n",
    "\n",
    "- [0,0] = 0.6441 (Your, Your)journey attending to \"Your\" paired with \"Your\"\n",
    "- [0,2] = 0.6217(Your, starts)journey attending to \"Your\" paired with \"starts\"\n",
    "- [1,2] = 1.0946(journey, starts)journey attending to \"journey\" paired with \"starts\"\n",
    "- [4,5] = 0.2315(one, step)journey attending to \"one\" paired with \"step\"\n",
    "\n",
    "```\n",
    "Standard:     query → token           (6 pairs)\n",
    "              journey → Your, journey → starts, ...\n",
    "\n",
    "2-Simplicial: query → (token, token)  (36 triplets)\n",
    "              journey → (Your, Your), journey → (Your, journey), journey → (Your, starts), ...\n",
    "\n",
    "```\n",
    "\n",
    "So instead of 6 attention scores, we have 36 attention scores - one for every possible pair of tokens that the query can attend to together!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e982e",
   "metadata": {},
   "source": [
    "#### Understanding the trilinear product (3 vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d458b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2-Simplicial Attention (trilinear product of 3 vectors) ===\n",
      "tensor(0.6217)\n",
      "tensor(0.6217)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 2-Simplicial Attention (trilinear product of 3 vectors) ===\")\n",
    "# For A[query, j=0, k=2] = trilinear(query, inputs[0], inputs[2])\n",
    "j = 0  # Your\n",
    "k = 2  # starts\n",
    "\n",
    "res = 0\n",
    "for idx in range(len(query)):\n",
    "    res += query[idx]* inputs[j][idx] * inputs[k][idx] # # element wise multiplication of 3 vectors and addition\n",
    "\n",
    "print(res)\n",
    "print(torch.sum(query * inputs[j] * inputs[k]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5152f",
   "metadata": {},
   "source": [
    "#### Standard Attention (dot product -2 vectors)\n",
    "\n",
    "```\n",
    "query = inputs[1] = [0.55, 0.87, 0.66]  (journey)\n",
    "inputs[0] = [0.43, 0.15, 0.89]  (Your)\n",
    "\n",
    "attn_score[0] = (0.43*0.55) + (0.15*0.87) + (0.89*0.66) = 0.9544\n",
    "```\n",
    "\n",
    "**Interpretation:** This is the attention score for inputs[0] (Your) with respect to inputs[1] (journey). It measures how much \"journey\" should attend to \"Your\".\n",
    "\n",
    "#### 2-Simplicial Attention (trilinear product - 3 vectors)\n",
    "\n",
    "```\n",
    "query = inputs[1] = [0.55, 0.87, 0.66]  (journey)\n",
    "inputs[0] = [0.43, 0.15, 0.89]  (Your)\n",
    "inputs[2] = [0.57, 0.85, 0.64]  (starts)\n",
    "\n",
    "attn_score_2s[0,2] = (0.55*0.43*0.57) + (0.87*0.15*0.85) + (0.66*0.89*0.64) = 0.6217\n",
    "```\n",
    "**Interpretation:** This is the attention score for the pair (inputs[0], inputs[2]) i.e. (Your, starts) with respect to inputs[1] (journey). It measures how much \"journey\" should attend to the combination of \"Your\" AND \"starts\" together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab017f8",
   "metadata": {},
   "source": [
    "### Comparision of Text vs Point Transformer\n",
    "\n",
    "Text Transformer Pipeline\n",
    "\n",
    "```\n",
    "Raw Text: \"Your journey starts with one step\"\n",
    "    ↓\n",
    "Tokenization: [token_1, token_2, token_3, token_4, token_5, token_6]\n",
    "    ↓\n",
    "Token Embedding Layer: nn.Embedding(vocab_size, d_model)\n",
    "    → Each token → lookup → embedding vector\n",
    "    ↓\n",
    "+ Positional Encoding: (RoPE / Sinusoidal / Learned)\n",
    "    ↓\n",
    "Features for Attention: [feat_1, feat_2, ..., feat_6]  shape: (6, d_model)\n",
    "\n",
    "```\n",
    "\n",
    "Point Transformer V3 Pipeline\n",
    "\n",
    "```\n",
    "Raw Point Cloud: N points with (x, y, z, r, g, b) or (x, y, z, intensity)\n",
    "    ↓\n",
    "Voxelization / Grid Sampling: \n",
    "    → coord: (N, 3) - 3D coordinates\n",
    "    → feat: (N, C) - initial features (RGB, intensity, normals)\n",
    "    ↓\n",
    "Serialization: (Z-order / Hilbert curve)\n",
    "    → Converts unordered points to ordered sequence\n",
    "    ↓\n",
    "Embedding Layer (Sparse Conv): \n",
    "    → Projects initial features to higher dimension\n",
    "    → Acts like \"token embedding\" for point clouds\n",
    "    → Also serves as positional encoding (xCPE - enhanced Conditional Position Encoding)\n",
    "    ↓\n",
    "Features for Attention: [feat_1, feat_2, ..., feat_N]  shape: (N, d_model)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042ad197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# === RAW POINT CLOUD INPUT ===\n",
    "# 6 points from a simple scene\n",
    "\n",
    "# coord: 3D spatial coordinates (x, y, z) - WHERE the point is in space\n",
    "coord = torch.tensor([\n",
    "    [0.0, 0.0, 0.0],   # Point 1: floor corner\n",
    "    [1.0, 0.0, 0.0],   # Point 2: floor edge        # query point\n",
    "    [1.0, 1.0, 0.0],   # Point 3: floor edge\n",
    "    [0.0, 0.0, 1.0],   # Point 4: wall corner\n",
    "    [1.0, 0.0, 1.0],   # Point 5: wall edge\n",
    "    [0.5, 0.5, 0.5]    # Point 6: mid-air point\n",
    "])\n",
    "\n",
    "# Initial features (e.g., RGB color, intensity) - raw input\n",
    "raw_feat = torch.tensor([\n",
    "    [0.8, 0.2, 0.1],   # Point 1: reddish\n",
    "    [0.7, 0.3, 0.2],   # Point 2: reddish\n",
    "    [0.6, 0.4, 0.3],   # Point 3: brownish\n",
    "    [0.9, 0.9, 0.9],   # Point 4: white (wall)\n",
    "    [0.85, 0.85, 0.85],# Point 5: white (wall)\n",
    "    [0.1, 0.5, 0.8]    # Point 6: bluish (object)\n",
    "])\n",
    "\n",
    "# === AFTER SPARSE CONV EMBEDDING (like token embedding + pos encoding) ===\n",
    "# In PTv3, a sparse conv layer projects raw_feat to higher dimension\n",
    "# and implicitly encodes positional information from coord\n",
    "\n",
    "# Simulated output after sparse conv embedding layer\n",
    "# This is what actually goes into attention!\n",
    "feat = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  # Point 1: embedded feature\n",
    "    [0.55, 0.87, 0.66],  # Point 2: embedded feature  # query point\n",
    "    [0.57, 0.85, 0.64],  # Point 3: embedded feature\n",
    "    [0.22, 0.58, 0.33],  # Point 4: embedded feature\n",
    "    [0.77, 0.25, 0.10],  # Point 5: embedded feature\n",
    "    [0.05, 0.80, 0.55]   # Point 6: embedded feature\n",
    "])\n",
    "\n",
    "# This \"feat\" is analogous to your text \"inputs\" tensor!\n",
    "# Now attention operates on these embedded features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c8d6a",
   "metadata": {},
   "source": [
    "![Point Transformer v3 vs Text data](/DATA/pyare/Routine/LLM/Reasoning/LLMs-from-scratch-pyare/chapter-3-coding-attention/Ref_images/PTv3_vs_Text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c23332",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84260b1e",
   "metadata": {},
   "source": [
    "#### 1-simplicial (Standard) Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485f2aa",
   "metadata": {},
   "source": [
    "![Attention weights](../Ref_images/3.9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c76eab",
   "metadata": {},
   "source": [
    "- We normalize each attention scores  we computed previously. The main goal behind the normalization is to obtain attention weights that sum upto 1.\n",
    "- This normalization is a convention that is useful for interpretation and maintaining training stability in an LLM. Here is a straight forward method for achieving this normalization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2755d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention scores: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "\n",
      "sum of attention scores: tensor(6.5617)\n",
      "\n",
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "\n",
      "Sum of Attention weights: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2/attn_scores_2.sum()\n",
    "print(\"\\nAttention scores:\", attn_scores_2)\n",
    "print(\"\\nsum of attention scores:\", attn_scores_2.sum())\n",
    "print(\"\\nAttention weights:\", attn_weights_2_tmp)\n",
    "print(\"\\nSum of Attention weights:\", attn_weights_2_tmp.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377e75c",
   "metadata": {},
   "source": [
    "#### For 2-simplicial Attention Weights\n",
    "\n",
    "- **Key Difference:** \n",
    "In 2-simplicial attention, we have a 2D matrix of scores (T × T), so we normalize over both dimensions (all T² values sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1f095416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention scores (2D matrix):\n",
      " tensor([[0.6441, 0.6313, 0.6217, 0.3216, 0.2735, 0.4393],\n",
      "        [0.6313, 1.1124, 1.0946, 0.6493, 0.4657, 0.8602],\n",
      "        [0.6217, 1.0946, 1.0776, 0.6373, 0.4685, 0.8396],\n",
      "        [0.3216, 0.6493, 0.6373, 0.3912, 0.2411, 0.5295],\n",
      "        [0.2735, 0.4657, 0.4685, 0.2411, 0.3871, 0.2315],\n",
      "        [0.4393, 0.8602, 0.8396, 0.5295, 0.2315, 0.7578]])\n",
      "\n",
      "Sum of attention scores: tensor(20.9792)\n",
      "\n",
      "Attention weights (2D matrix):\n",
      " tensor([[0.0307, 0.0301, 0.0296, 0.0153, 0.0130, 0.0209],\n",
      "        [0.0301, 0.0530, 0.0522, 0.0309, 0.0222, 0.0410],\n",
      "        [0.0296, 0.0522, 0.0514, 0.0304, 0.0223, 0.0400],\n",
      "        [0.0153, 0.0309, 0.0304, 0.0186, 0.0115, 0.0252],\n",
      "        [0.0130, 0.0222, 0.0223, 0.0115, 0.0185, 0.0110],\n",
      "        [0.0209, 0.0410, 0.0400, 0.0252, 0.0110, 0.0361]])\n",
      "\n",
      "Sum of Attention weights: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 2-Simplicial attention scores (6x6 matrix)\n",
    "attn_scores_2s = torch.empty(inputs.shape[0], inputs.shape[0])\n",
    "for j, k_j in enumerate(inputs):\n",
    "    for k, k_prime_k in enumerate(inputs):\n",
    "        attn_scores_2s[j, k] = torch.sum(query * k_j * k_prime_k)\n",
    "\n",
    "# Normalize: divide by sum of ALL elements in the matrix\n",
    "attn_weights_2s_tmp = attn_scores_2s / attn_scores_2s.sum()\n",
    "\n",
    "print(\"\\nAttention scores (2D matrix):\\n\", attn_scores_2s)\n",
    "print(\"\\nSum of attention scores:\", attn_scores_2s.sum())\n",
    "print(\"\\nAttention weights (2D matrix):\\n\", attn_weights_2s_tmp)\n",
    "print(\"\\nSum of Attention weights:\", attn_weights_2s_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353558d",
   "metadata": {},
   "source": [
    "#### **Softmax Normalization**: \n",
    "In Practice, its more common and advisable to use softmax function for normalization. This approach is better at managing extreme values and offer more favorable gradient properties during training. The following is a basic implementation of the softmax funtion for normalizing the attention scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9782301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02b59877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "\n",
      "Attention weights Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\",attn_weights_2_naive)\n",
    "print(\"\\nAttention weights Sum:\",attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d446e1f",
   "metadata": {},
   "source": [
    "#### **Softmax Normalization for 2-Simplicial Attention:**\n",
    "\n",
    "- For 2-simplicial attention, we apply softmax over both dimensions (flatten the matrix, apply softmax, reshape back). This ensures all T×T weights sum to 1.\n",
    "\n",
    "**Why Flatten?**\n",
    "\n",
    "- In standard attention: query chooses among T tokens --> softmax over T options.\n",
    "- In 2-Simplicial: query chooses among T*T pairs --> softmax over $T^2$ options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebe8c737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights (2D matrix):\n",
      " tensor([[0.0285, 0.0282, 0.0279, 0.0207, 0.0197, 0.0233],\n",
      "        [0.0282, 0.0456, 0.0448, 0.0287, 0.0239, 0.0354],\n",
      "        [0.0279, 0.0448, 0.0440, 0.0283, 0.0239, 0.0347],\n",
      "        [0.0207, 0.0287, 0.0283, 0.0222, 0.0191, 0.0254],\n",
      "        [0.0197, 0.0239, 0.0239, 0.0191, 0.0221, 0.0189],\n",
      "        [0.0233, 0.0354, 0.0347, 0.0254, 0.0189, 0.0320]])\n",
      "\\Attention weights Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive_2d(x):\n",
    "    # Flatten the 2D matrix, apply softmax, reshape back\n",
    "    flat = x.flatten()\n",
    "    softmax_flat = torch.exp(flat) / torch.exp(flat).sum()\n",
    "    return softmax_flat.view(x.shape)\n",
    "\n",
    "attn_weights_2s_naive = softmax_naive_2d(attn_scores_2s)\n",
    "\n",
    "print(\"Attention weights (2D matrix):\\n\", attn_weights_2s_naive)\n",
    "print(\"\\Attention weights Sum:\",attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5102d60",
   "metadata": {},
   "source": [
    "- Softmax function also meets the objective and normalize the attention weights such that they sum to 1.\n",
    "-  In addition, the softmax function ensure that attention weights are always possitive. This makes the output interpretable as probilities or relative importance, where higher weights indicate greater importance.\n",
    "- **Note**: This naive softmax implementation (softmax_naive) may enconter the numerical instability problem.such as overflow and underflow, when dealing with large or small input values. Therefore it is advisable to use the Pytorch implementation of softmax, which has been extensively optimized for performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "210ba4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "\n",
      "Attention weights Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_torch = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\",attn_weights_2_torch)\n",
    "print(\"\\nAttention weights Sum:\",attn_weights_2_torch.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6f536",
   "metadata": {},
   "source": [
    "- In this case, it yields the same results as our previous softmax_naive function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe4dc6",
   "metadata": {},
   "source": [
    "#### PyTorch Softmax for 2-Simplicial Attention\n",
    "- **Note:** The naive softmax implementation (softmax_naive_2d) may encounter numerical instability problems such as overflow and underflow when dealing with large or small input values. Therefore it is advisable to use the PyTorch implementation of softmax, which has been extensively optimized for performance.\n",
    "- **Key Difference:** For 2-simplicial attention, we need to flatten the 2D matrix before applying softmax, then reshape back. We use dim=None by flattening first, or reshape and use dim=-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cacd75c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights (2D matrix):\n",
      " tensor([[0.0285, 0.0282, 0.0279, 0.0207, 0.0197, 0.0233],\n",
      "        [0.0282, 0.0456, 0.0448, 0.0287, 0.0239, 0.0354],\n",
      "        [0.0279, 0.0448, 0.0440, 0.0283, 0.0239, 0.0347],\n",
      "        [0.0207, 0.0287, 0.0283, 0.0222, 0.0191, 0.0254],\n",
      "        [0.0197, 0.0239, 0.0239, 0.0191, 0.0221, 0.0189],\n",
      "        [0.0233, 0.0354, 0.0347, 0.0254, 0.0189, 0.0320]])\n",
      "Attention weights Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2s_torch = torch.softmax(attn_scores_2s.flatten(),dim=0).view(attn_scores_2s.shape)\n",
    "print(\"Attention weights (2D matrix):\\n\",attn_weights_2s_torch)\n",
    "print(\"Attention weights Sum:\", attn_weights_2s_torch.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c0704",
   "metadata": {},
   "source": [
    "#### **Stable Softmax Normalization**: (ICLR'25)\n",
    "\n",
    "- Helps in groking (delayed generalization), [Paper Link](https://arxiv.org/pdf/2501.04697). As we have discussed there is problem of overflow and underflow in case of large and small values of input.\n",
    "- [Github Link](https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability)\n",
    "\n",
    "    ![Stable max](../Ref_images/Stable_max.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280212e9",
   "metadata": {},
   "source": [
    "- both the above versions can be used\n",
    "    - For faster implementation use linear transformation (x+1) and 1/(1-x)\n",
    "    - For better compatibility use log(x+1) and -log(-x+1) as later we want to use other properties of softmax.\n",
    "    - Here we will implement only the log version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a134928e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1556, 0.1986, 0.1971, 0.1467, 0.1359, 0.1661])\n",
      "\n",
      "Attention weights Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    return torch.where(\n",
    "        x>=0,\n",
    "        torch.log(x+1),\n",
    "        -torch.log(-x+1)\n",
    "    )\n",
    "attn_weights_2_stablemax = torch.softmax(g(attn_scores_2), dim=0)\n",
    "print(\"Attention weights:\",attn_weights_2_stablemax)\n",
    "print(\"\\nAttention weights Sum:\",attn_weights_2_stablemax.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97adc725",
   "metadata": {},
   "source": [
    "#### **Stable Softmax Normalization for 2-Simplicial Attention:** (ICLR'25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d507863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights (2D matrix):\n",
      " tensor([[0.0289, 0.0286, 0.0285, 0.0232, 0.0223, 0.0253],\n",
      "        [0.0286, 0.0371, 0.0368, 0.0289, 0.0257, 0.0326],\n",
      "        [0.0285, 0.0368, 0.0365, 0.0287, 0.0258, 0.0323],\n",
      "        [0.0232, 0.0289, 0.0287, 0.0244, 0.0218, 0.0268],\n",
      "        [0.0223, 0.0257, 0.0258, 0.0218, 0.0243, 0.0216],\n",
      "        [0.0253, 0.0326, 0.0323, 0.0268, 0.0216, 0.0309]])\n",
      "\n",
      "Attention weights Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    return torch.where(\n",
    "        x >= 0,\n",
    "        torch.log(x + 1),\n",
    "        -torch.log(-x + 1)\n",
    "    )\n",
    "\n",
    "# Apply g() to the 2D scores matrix, then flatten, softmax, reshape\n",
    "attn_weights_2s_stablemax = torch.softmax(g(attn_scores_2s).flatten(), dim = 0).view(attn_scores_2s.shape)\n",
    "\n",
    "print(\"Attention weights (2D matrix):\\n\",attn_weights_2s_stablemax)\n",
    "print(\"\\nAttention weights Sum:\", attn_weights_2_stablemax.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fd1ac",
   "metadata": {},
   "source": [
    "**Why stable softmax matters for 2-Simplicial:**\n",
    "\n",
    "- 2-simplicial has T×T scores instead of T scores\n",
    "- More values → higher chance of extreme values\n",
    "- Trilinear products can produce larger/smaller values than bilinear (dot product)\n",
    "- Stable transformation g(x) compresses the range before softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cadf47",
   "metadata": {},
   "source": [
    "## **Context Vector**:\n",
    "Now that we have computed the normalized attention weights, we are ready for the final step, as shown in the figure 3.10, calculating the **Context Vector** $z(2)$ by multiplying the embedded input tokens, $x(i)$ with the corresonding attention weights and then summing the resulting vectors.\n",
    "\n",
    "- Thus, Context vector $z(2)$ is weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:\n",
    "\n",
    "![3.10](../Ref_images/3.10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7df568b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Context vector with torch softmax\n",
    "query = inputs[1] # The second input token is the query\n",
    "\n",
    "context_vec_2_torch = torch.zeros(query.shape) # query.shape: torch.Size([3])\n",
    "for i, x_i in enumerate(inputs): # going throgh whole input ( 0 to 5)\n",
    "    context_vec_2_torch += attn_weights_2_torch[i]*x_i\n",
    "\n",
    "print(context_vec_2_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e22d3",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "i | attn_weight | input_vector          | Weight × Vector\n",
    "--+-------------+-----------------------+-------------------------\n",
    "0 | 0.1385      | [0.43, 0.15, 0.89]   | [0.0595, 0.0207, 0.1232]\n",
    "1 | 0.2379      | [0.55, 0.87, 0.66]   | [0.1308, 0.2069, 0.1570]\n",
    "2 | 0.2333      | [0.57, 0.85, 0.64]   | [0.1329, 0.1983, 0.1493]\n",
    "3 | 0.1240      | [0.22, 0.58, 0.33]   | [0.0272, 0.0719, 0.0409]\n",
    "4 | 0.1082      | [0.77, 0.25, 0.10]   | [0.0833, 0.0270, 0.0108]\n",
    "5 | 0.1581      | [0.05, 0.80, 0.55]   | [0.0079, 0.1264, 0.0869]\n",
    "--+-------------+-----------------------+---------------------------\n",
    "  | SUM         |                      | [0.4419, 0.6515, 0.5683]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0195f96",
   "metadata": {},
   "source": [
    "*Manual Calculations*: for better understandng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb9553ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4337, 0.6156, 0.5490])\n"
     ]
    }
   ],
   "source": [
    "# Context vector with torch stablemax\n",
    "query = inputs[1] # The second input token is the query\n",
    "\n",
    "context_vec_2_satblemax = torch.zeros(query.shape) # query.shape: torch.Size([3])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2_satblemax += attn_weights_2_stablemax[i]*x_i\n",
    "\n",
    "print(context_vec_2_satblemax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d385b",
   "metadata": {},
   "source": [
    "### Computing Attention Weights for all inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae2b6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c30b6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1e978c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
