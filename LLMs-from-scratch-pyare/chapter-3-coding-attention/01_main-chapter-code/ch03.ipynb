{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f22086",
   "metadata": {},
   "source": [
    "# Chapter 03 : Coding Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70dcdb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\" torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9ed3a",
   "metadata": {},
   "source": [
    "\n",
    "Workflow till now\n",
    "\n",
    "<div align = \"center\">\n",
    "    <img src = \"/DATA/pyare/Routine/LLM/Reasoning/LLMs-from-scratch-pyare/chapter-3-coding-attention/Ref_images/3.0.png\" center width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9073dde",
   "metadata": {},
   "source": [
    "- In this chapter we will code \n",
    "\n",
    "<img src=\"/DATA/pyare/Routine/LLM/Reasoning/LLMs-from-scratch-pyare/chapter-3-coding-attention/Ref_images/3.2.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365de207",
   "metadata": {},
   "source": [
    "### 3.3.1 A Simple self-attention mechanism without trainable weights\n",
    "<img src =\"/DATA/pyare/Routine/LLM/Reasoning/LLMs-from-scratch-pyare/chapter-3-coding-attention/Ref_images/3.7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a43f10",
   "metadata": {},
   "source": [
    "- Input sequence $x$, consisting of $T$ elements represented as $x^{(1)}$ to $x^{(T)}$\n",
    "\n",
    "Example: \"Your journey starts with one step\"\n",
    "- $x^{(1)}$, corresponds to a d-dimentional embedding vector representing a specific token, like \"Your\" has 3 dimentional embeddings.\n",
    "\n",
    "In **self-attention** your goal is to calculate context vectors $z^{(i)}$ for each element in the input sequence.\n",
    "\n",
    "- A ***Context Vector*** can be interpreted as an enriched embedding vector.\n",
    "\n",
    "- We will illustrate this with taking the word \"journey\" $x^{(2)}$ and the corresponding context vector $z^{(2)}$ which contains the infrmation about $x^{(2)}$ and all other input elements, $x^{(1)}$ to $x^{(T)}$.\n",
    "\n",
    "- First we will do simplified attention and later we will add trainable weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2) # query\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1448ff",
   "metadata": {},
   "source": [
    "- First step of implementing self-attention is to comppute the intermediate values *w*, referred as ***attention scores***.\n",
    "\n",
    "<img src=\"/DATA/pyare/Routine/LLM/Reasoning/LLMs-from-scratch-pyare/chapter-3-coding-attention/Ref_images/3.8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf204ff9",
   "metadata": {},
   "source": [
    "### Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d7d6c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] #  journey  (x^2)\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0]) # \n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111e39d",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
    "        x^1     x^2     x^3     x^4     x^5     x^6\n",
    "        Your   journey starts   with    one    step\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02a75f",
   "metadata": {},
   "source": [
    "### Understanding the dot product (bilinear - 2 vectors)\n",
    "An dot product is essentially a concise way of multiplying two vectors element wise and then summing the products. Which is demonstrated as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f94efbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0] # Yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6556206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Standard Attention (dot product of 2 vectors) ===\n",
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "### Understanding the dot product (bilinear - 2 vectors)\n",
    "print(\"=== Standard Attention (dot product of 2 vectors) ===\")\n",
    "res = 0\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx]*query[idx] # element wise multiplication and addition\n",
    "print(res)\n",
    "print(torch.dot(inputs[0],query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c1eb2",
   "metadata": {},
   "source": [
    "#### 2-simplicial Attention scores: 2D scores (for single query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4ebe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2-Simplicial attention scores: \n",
      "tensor([[0.6441, 0.6313, 0.6217, 0.3216, 0.2735, 0.4393],\n",
      "        [0.6313, 1.1124, 1.0946, 0.6493, 0.4657, 0.8602],\n",
      "        [0.6217, 1.0946, 1.0776, 0.6373, 0.4685, 0.8396],\n",
      "        [0.3216, 0.6493, 0.6373, 0.3912, 0.2411, 0.5295],\n",
      "        [0.2735, 0.4657, 0.4685, 0.2411, 0.3871, 0.2315],\n",
      "        [0.4393, 0.8602, 0.8396, 0.5295, 0.2315, 0.7578]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2s = torch.empty(inputs.shape[0],inputs.shape[0]) # 6*6 = number of tokes * number of tokens\n",
    "for j, k_j in enumerate(inputs):\n",
    "    for k, k_prime_k in enumerate(inputs):\n",
    "        attn_scores_2s[j,k] = torch.sum(query* k_j* k_prime_k)\n",
    "\n",
    "print(\"\\n2-Simplicial attention scores: \")\n",
    "print(attn_scores_2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3790c",
   "metadata": {},
   "source": [
    "```\n",
    "                k=0     k=1     k=2     k=3     k=4     k=5\n",
    "                Your   journey starts   with    one    step\n",
    "            ┌──────────────────────────────────────────────  ┐\n",
    "j=0  Your   │ 0.6441  0.6313  0.6217  0.3216  0.2735  0.4393 │\n",
    "j=1 journey │ 0.6313  1.1124  1.0946  0.6493  0.4657  0.8602 │\n",
    "j=2 starts  │ 0.6217  1.0946  1.0776  0.6373  0.4685  0.8396 │\n",
    "j=3  with   │ 0.3216  0.6493  0.6373  0.3912  0.2411  0.5295 │\n",
    "j=4  one    │ 0.2735  0.4657  0.4685  0.2411  0.3871  0.2315 │\n",
    "j=5  step   │ 0.4393  0.8602  0.8396  0.5295  0.2315  0.7578 │\n",
    "            └──────────────────────────────────────────────  ┘\n",
    "```\n",
    "\n",
    "**Interpretation:** 36 scores (6×6) representing how much query (journey) attends to each pair of tokens.\n",
    "\n",
    "- [0,0] = 0.6441 (Your, Your)journey attending to \"Your\" paired with \"Your\"\n",
    "- [0,2] = 0.6217(Your, starts)journey attending to \"Your\" paired with \"starts\"\n",
    "- [1,2] = 1.0946(journey, starts)journey attending to \"journey\" paired with \"starts\"\n",
    "- [4,5] = 0.2315(one, step)journey attending to \"one\" paired with \"step\"\n",
    "\n",
    "```\n",
    "Standard:     query → token           (6 pairs)\n",
    "              journey → Your, journey → starts, ...\n",
    "\n",
    "2-Simplicial: query → (token, token)  (36 triplets)\n",
    "              journey → (Your, Your), journey → (Your, journey), journey → (Your, starts), ...\n",
    "\n",
    "```\n",
    "\n",
    "So instead of 6 attention scores, we have 36 attention scores - one for every possible pair of tokens that the query can attend to together!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e982e",
   "metadata": {},
   "source": [
    "#### Understanding the trilinear product (3 vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d458b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2-Simplicial Attention (trilinear product of 3 vectors) ===\n",
      "tensor(0.6217)\n",
      "tensor(0.6217)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 2-Simplicial Attention (trilinear product of 3 vectors) ===\")\n",
    "# For A[query, j=0, k=2] = trilinear(query, inputs[0], inputs[2])\n",
    "j = 0  # Your\n",
    "k = 2  # starts\n",
    "\n",
    "res = 0\n",
    "for idx in range(len(query)):\n",
    "    res += query[idx]* inputs[j][idx] * inputs[k][idx] # # element wise multiplication of 3 vectors and addition\n",
    "\n",
    "print(res)\n",
    "print(torch.sum(query * inputs[j] * inputs[k]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5152f",
   "metadata": {},
   "source": [
    "#### Standard Attention (dot product -2 vectors)\n",
    "\n",
    "```\n",
    "query = inputs[1] = [0.55, 0.87, 0.66]  (journey)\n",
    "inputs[0] = [0.43, 0.15, 0.89]  (Your)\n",
    "\n",
    "attn_score[0] = (0.43*0.55) + (0.15*0.87) + (0.89*0.66) = 0.9544\n",
    "```\n",
    "\n",
    "**Interpretation:** This is the attention score for inputs[0] (Your) with respect to inputs[1] (journey). It measures how much \"journey\" should attend to \"Your\".\n",
    "\n",
    "#### 2-Simplicial Attention (trilinear product - 3 vectors)\n",
    "\n",
    "```\n",
    "query = inputs[1] = [0.55, 0.87, 0.66]  (journey)\n",
    "inputs[0] = [0.43, 0.15, 0.89]  (Your)\n",
    "inputs[2] = [0.57, 0.85, 0.64]  (starts)\n",
    "\n",
    "attn_score_2s[0,2] = (0.55*0.43*0.57) + (0.87*0.15*0.85) + (0.66*0.89*0.64) = 0.6217\n",
    "```\n",
    "**Interpretation:** This is the attention score for the pair (inputs[0], inputs[2]) i.e. (Your, starts) with respect to inputs[1] (journey). It measures how much \"journey\" should attend to the combination of \"Your\" AND \"starts\" together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab017f8",
   "metadata": {},
   "source": [
    "### Comparision of Text vs Point Transformer\n",
    "\n",
    "Text Transformer Pipeline\n",
    "\n",
    "```\n",
    "Raw Text: \"Your journey starts with one step\"\n",
    "    ↓\n",
    "Tokenization: [token_1, token_2, token_3, token_4, token_5, token_6]\n",
    "    ↓\n",
    "Token Embedding Layer: nn.Embedding(vocab_size, d_model)\n",
    "    → Each token → lookup → embedding vector\n",
    "    ↓\n",
    "+ Positional Encoding: (RoPE / Sinusoidal / Learned)\n",
    "    ↓\n",
    "Features for Attention: [feat_1, feat_2, ..., feat_6]  shape: (6, d_model)\n",
    "\n",
    "```\n",
    "\n",
    "Point Transformer V3 Pipeline\n",
    "\n",
    "```\n",
    "Raw Point Cloud: N points with (x, y, z, r, g, b) or (x, y, z, intensity)\n",
    "    ↓\n",
    "Voxelization / Grid Sampling: \n",
    "    → coord: (N, 3) - 3D coordinates\n",
    "    → feat: (N, C) - initial features (RGB, intensity, normals)\n",
    "    ↓\n",
    "Serialization: (Z-order / Hilbert curve)\n",
    "    → Converts unordered points to ordered sequence\n",
    "    ↓\n",
    "Embedding Layer (Sparse Conv): \n",
    "    → Projects initial features to higher dimension\n",
    "    → Acts like \"token embedding\" for point clouds\n",
    "    → Also serves as positional encoding (xCPE - enhanced Conditional Position Encoding)\n",
    "    ↓\n",
    "Features for Attention: [feat_1, feat_2, ..., feat_N]  shape: (N, d_model)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042ad197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# === RAW POINT CLOUD INPUT ===\n",
    "# 6 points from a simple scene\n",
    "\n",
    "# coord: 3D spatial coordinates (x, y, z) - WHERE the point is in space\n",
    "coord = torch.tensor([\n",
    "    [0.0, 0.0, 0.0],   # Point 1: floor corner\n",
    "    [1.0, 0.0, 0.0],   # Point 2: floor edge        # query point\n",
    "    [1.0, 1.0, 0.0],   # Point 3: floor edge\n",
    "    [0.0, 0.0, 1.0],   # Point 4: wall corner\n",
    "    [1.0, 0.0, 1.0],   # Point 5: wall edge\n",
    "    [0.5, 0.5, 0.5]    # Point 6: mid-air point\n",
    "])\n",
    "\n",
    "# Initial features (e.g., RGB color, intensity) - raw input\n",
    "raw_feat = torch.tensor([\n",
    "    [0.8, 0.2, 0.1],   # Point 1: reddish\n",
    "    [0.7, 0.3, 0.2],   # Point 2: reddish\n",
    "    [0.6, 0.4, 0.3],   # Point 3: brownish\n",
    "    [0.9, 0.9, 0.9],   # Point 4: white (wall)\n",
    "    [0.85, 0.85, 0.85],# Point 5: white (wall)\n",
    "    [0.1, 0.5, 0.8]    # Point 6: bluish (object)\n",
    "])\n",
    "\n",
    "# === AFTER SPARSE CONV EMBEDDING (like token embedding + pos encoding) ===\n",
    "# In PTv3, a sparse conv layer projects raw_feat to higher dimension\n",
    "# and implicitly encodes positional information from coord\n",
    "\n",
    "# Simulated output after sparse conv embedding layer\n",
    "# This is what actually goes into attention!\n",
    "feat = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  # Point 1: embedded feature\n",
    "    [0.55, 0.87, 0.66],  # Point 2: embedded feature  # query point\n",
    "    [0.57, 0.85, 0.64],  # Point 3: embedded feature\n",
    "    [0.22, 0.58, 0.33],  # Point 4: embedded feature\n",
    "    [0.77, 0.25, 0.10],  # Point 5: embedded feature\n",
    "    [0.05, 0.80, 0.55]   # Point 6: embedded feature\n",
    "])\n",
    "\n",
    "# This \"feat\" is analogous to your text \"inputs\" tensor!\n",
    "# Now attention operates on these embedded features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c8d6a",
   "metadata": {},
   "source": [
    "![Point Transformer v3 vs Text data](/DATA/pyare/Routine/LLM/Reasoning/LLMs-from-scratch-pyare/chapter-3-coding-attention/Ref_images/PTv3_vs_Text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84260b1e",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c76eab",
   "metadata": {},
   "source": [
    "- We normalize each attention scores  we computed previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2755d78",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
