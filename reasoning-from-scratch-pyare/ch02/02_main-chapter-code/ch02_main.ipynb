{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc141850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version : 2.9.1+cu128\n",
      "CUDA GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.cpu import is_available\n",
    "\n",
    "print(f\"Pytorch version : {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA GPU\")\n",
    "elif torch.mps.is_available():\n",
    "    print(\"Apple Silicon GPU\") \n",
    "else:\n",
    "    print(\"CPU Only\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637344f",
   "metadata": {},
   "source": [
    "--> Packages that will be being used in this book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5f0bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning_from_scratch verion : 0.1.8\n",
      "torch verion : 2.9.1\n",
      "tokenizers verion : 0.22.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "used_libraries = [\n",
    "    \"reasoning_from_scratch\",\n",
    "    \"torch\",\n",
    "    \"tokenizers\" \n",
    "]\n",
    "\n",
    "for lib in used_libraries:\n",
    "    print(f\"{lib} verion : {version(lib)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f7be9",
   "metadata": {},
   "source": [
    "### Using the Tensor cores\n",
    "- Important to note that If you have moderen Voltas architecture or newer you can take advantage of Tensor cores, which are specialized in the matrix multiplications.\n",
    "To enable them simply execute the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a292544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is enforcing the float32 for matrix multiplication., by default torch set it to highest : read more at https://docs.pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "# Better way is to use the AMP ( Automatic Mixed Precision)\n",
    "# Have a look on the template related to AMP : /DATA/pyare/Routine/LLM/Reasoning/reasoning-from-scratch-pyare/ch02/02_main-chapter-code/notes/single_gpu_AMP_training_template.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58dc531",
   "metadata": {},
   "source": [
    "## 2.4 Preparing the Input Text for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11927dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import the tokenizer\n",
    "from reasoning_from_scratch.qwen3 import download_qwen3_small\n",
    "download_qwen3_small(kind=\"base\", tokenizer_only=True, out_dir = \"qwen3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d7f6c0",
   "metadata": {},
   "source": [
    "The cammand downloads the `tokenizer-base.json` file. Now we can load the tokenizer settings from the tokenizer file into the `Qwen3Tokenizer`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965b5501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3790953/2524807295.py:2: DeprecationWarning: module 'sre_parse' is deprecated\n",
      "  from sre_parse import Tokenizer\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sre_parse import Tokenizer \n",
    "from reasoning_from_scratch.qwen3 import Qwen3Tokenizer\n",
    "\n",
    "tokenizer_path = Path(\"qwen3\")/\"tokenizer-base.json\" \n",
    "tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe2fd2",
   "metadata": {},
   "source": [
    "<img src = \"/DATA/pyare/Routine/LLM/Reasoning/reasoning-from-scratch-pyare/ch02/02_main-chapter-code/notes/Notes_Images/02__image006.png\" alt =\"3\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3adbca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain large language models.\"\n",
    "input_token_ids_list = tokenizer.encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad7f954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain large language models.\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(input_token_ids_list)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e47792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[840] --> Ex\n",
      "[20772] --> plain\n",
      "[3460] -->  large\n",
      "[4128] -->  language\n",
      "[4119] -->  models\n",
      "[13] --> .\n"
     ]
    }
   ],
   "source": [
    "# lets look at the token ids\n",
    "for i in input_token_ids_list:\n",
    "    print(f\"{[i]} --> {tokenizer.decode([i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b4ade",
   "metadata": {},
   "source": [
    "- Explain --> Split into the Ex and Plain and it depends on the tokenizer algorithem here we used the Byte Pair Encoding (BPE) which is subword based method.\n",
    "- BPE can represent both common and rare words using a mix of full words and subword units.\n",
    "- Spaces are also often included in tokens. For example , \"large\", which often helps the LLM dectect word boundaries.\n",
    "- Quen3Tokenizer has a vocabulary of about 151,000 tokens, which is considered relatively large as of this writting ( for comparision, the early GPT-2 has vocabulary size of approximately 50,000 tokens and Llama 3 has a vocabulary size of approximately 128,000 tokens.)\n",
    "- More tokens means increase in size and computationals cost, nearly doubling the tokens approximately doubles the computational cost of running the model as it needs to generate more tokens to complete the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6172c",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Encoding unknown words\n",
    "Experiment with the tokenizer to see if and how it handles unknown words. For this, get creative and make up words that don't exist. Also, if you speak multiple languages, try to encode words in a different language than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LAter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de088d",
   "metadata": {},
   "source": [
    "## 2.5 Loading pre-trained models.\n",
    "- In this chapter we will use the 0.6B Qwen3 pre-trained as base model.\n",
    "Why Qwen3? \n",
    "- `Qwen3 0.6B` is more memory-efficient compared to `Llama 3 1B` and `OLMo 2 1B`.\n",
    "- custom reimplementation of the Qwen3 and are compatible with original implementation pre-trained Qwen3 model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62909ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
